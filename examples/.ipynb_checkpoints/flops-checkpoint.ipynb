{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81128264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from libs.layers import linear_attention\n",
    "from libs.layers import causal_linear_attn\n",
    "from libs.layers import attention\n",
    "from torch.nn.init import xavier_uniform_, constant_, xavier_normal_\n",
    "import copy\n",
    "import gc\n",
    "torch.cuda.set_device(4)\n",
    "from torch.nn.parameter import Parameter\n",
    "import time\n",
    "# import sys\n",
    "# sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272e35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class original_SimpleAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model,\n",
    "                 pos_dim: int = 1,\n",
    "                 attention_type='fourier',\n",
    "                 dropout=0.1,\n",
    "                 xavier_init=1e-4,\n",
    "                 diagonal_weight=1e-2,\n",
    "                 symmetric_init=False,\n",
    "                 norm=False,\n",
    "                 norm_type='layer',\n",
    "                 eps=1e-5,\n",
    "                 debug=False\n",
    "                ):\n",
    "        super(original_SimpleAttention, self).__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.attention_type = attention_type\n",
    "        self.d_k = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        self.pos_dim = pos_dim\n",
    "        self.linears = nn.ModuleList(\n",
    "            [copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(3)])\n",
    "        self.xavier_init = xavier_init\n",
    "        self.diagonal_weight = diagonal_weight\n",
    "        self.symmetric_init = symmetric_init\n",
    "        if self.xavier_init > 0:\n",
    "            self._reset_parameters()\n",
    "        self.add_norm = norm\n",
    "        self.norm_type = norm_type\n",
    "        if norm:\n",
    "            self._get_norm(eps=eps)\n",
    "\n",
    "        if pos_dim > 0:\n",
    "            self.fc = nn.Linear(d_model + n_head*pos_dim, d_model)\n",
    "\n",
    "        self.attn_weight = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, query, key, value, pos=None, mask=None, weight=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        bsz = query.size(0)\n",
    "        if weight is not None:\n",
    "            query, key = weight*query, weight*key\n",
    "\n",
    "        query, key, value = \\\n",
    "            [layer(x).view(bsz, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "             for layer, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "\n",
    "        if self.add_norm:\n",
    "            if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                value = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_V, (value[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "            else:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), query.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                query = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_Q, (query[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "\n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            assert pos.size(-1) == self.pos_dim\n",
    "            pos = pos.unsqueeze(1)\n",
    "            pos = pos.repeat([1,self.n_head, 1, 1])\n",
    "            query, key, value = [torch.cat([pos, x], dim=-1)\n",
    "                                  for x in (query, key, value)]\n",
    "    \n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            x, self.attn_weight = linear_attention(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   attention_type=self.attention_type,\n",
    "                                                   dropout=self.dropout)\n",
    "        elif self.attention_type == 'causal':\n",
    "            assert mask is not None\n",
    "            x, self.attn_weight = causal_linear_attn(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   dropout=self.dropout)\n",
    "        else:\n",
    "            x, self.attn_weight = attention(query, key, value,\n",
    "                                            mask=mask,\n",
    "                                            attention_type=self.attention_type,\n",
    "                                            dropout=self.dropout)\n",
    "\n",
    "        out_dim = self.n_head * self.d_k if pos is None else self.n_head * \\\n",
    "            (self.d_k + self.pos_dim)\n",
    "        att_output = x.transpose(1, 2).contiguous().view(bsz, -1, out_dim)\n",
    "\n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            att_output = self.fc(att_output)\n",
    "            \n",
    "       \n",
    "        return att_output, self.attn_weight\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for param in self.linears.parameters():\n",
    "            if param.ndim > 1:\n",
    "                xavier_uniform_(param, gain=self.xavier_init)\n",
    "                if self.diagonal_weight > 0.0:\n",
    "                    param.data += self.diagonal_weight * \\\n",
    "                        torch.diag(torch.ones(\n",
    "                            param.size(-1), dtype=torch.float))\n",
    "                if self.symmetric_init:\n",
    "                    param.data += param.data.T\n",
    "                    # param.data /= 2.0\n",
    "            else:\n",
    "                constant_(param, 0)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    def _get_norm(self, eps):\n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_V = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_V = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "        else:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_Q = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_Q = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_layernorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.LayerNorm(normalized_dim, **kwargs)) for _ in range(n_head)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_instancenorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.InstanceNorm1d(normalized_dim, **kwargs)) for _ in range(n_head)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2b4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_SimpleAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model,k,\n",
    "                 pos_dim: int = 1,\n",
    "                 attention_type='fourier',\n",
    "                 dropout=0.1,\n",
    "                 xavier_init=1e-4,\n",
    "                 diagonal_weight=1e-2,\n",
    "                 symmetric_init=False,\n",
    "                 norm=False,\n",
    "                 norm_type='layer',\n",
    "                 eps=1e-5,\n",
    "                 debug=False,\n",
    "                 seq_len = 2048,\n",
    "                ):\n",
    "        super(lin_SimpleAttention, self).__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.attention_type = attention_type\n",
    "        self.d_k = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        self.pos_dim = pos_dim\n",
    "        self.linears = nn.ModuleList(\n",
    "            [copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(3)])\n",
    "        self.xavier_init = xavier_init\n",
    "        self.diagonal_weight = diagonal_weight\n",
    "        self.symmetric_init = symmetric_init\n",
    "        if self.xavier_init > 0:\n",
    "            self._reset_parameters()\n",
    "        self.add_norm = norm\n",
    "        self.norm_type = norm_type\n",
    "        if norm:\n",
    "            self._get_norm(eps=eps)\n",
    "\n",
    "        if pos_dim > 0:\n",
    "            self.fc = nn.Linear(d_model + n_head*pos_dim, d_model)\n",
    "\n",
    "        self.attn_weight = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.E_weight = Parameter(torch.Tensor(seq_len, k))\n",
    "        self.F_weight = Parameter(torch.Tensor(seq_len, k))\n",
    "        self.E_bias = Parameter(torch.Tensor(k, 1))\n",
    "        self.F_bias = Parameter(torch.Tensor(k, 1))\n",
    "        xavier_normal_(self.E_weight)\n",
    "        xavier_normal_(self.F_weight)\n",
    "        xavier_normal_(self.E_bias)\n",
    "        xavier_normal_(self.F_bias) \n",
    "#         self.E = nn.Linear( seq_len, k)\n",
    "#         self.F = nn.Linear(seq_len, k)\n",
    "\n",
    "    def forward(self, query, key, value, pos=None, mask=None, weight=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        bsz = query.size(0)\n",
    "        if weight is not None:\n",
    "            query, key = weight*query, weight*key\n",
    "\n",
    "        query, key, value = \\\n",
    "            [layer(x).view(bsz, -1, self.n_head, self.d_k).transpose(1, 2)      ### bsz*n_head*n*dim\n",
    "             for layer, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "\n",
    "        if self.add_norm:\n",
    "            if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                value = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_V, (value[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "            else:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), query.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                query = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_Q, (query[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "\n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            assert pos.size(-1) == self.pos_dim\n",
    "            pos = pos.unsqueeze(1)\n",
    "            pos = pos.repeat([1, self.n_head, 1, 1])\n",
    "            query, key, value = [torch.cat([pos, x], dim=-1)     ### bsz*n_head*n*(dim+1)\n",
    "                                  for x in (query, key, value)]\n",
    " \n",
    "        key  = torch.einsum(\"ijkl,kz->ijzl\",key, self.E_weight)+ self.E_bias\n",
    "        value  = torch.einsum(\"ijkl,kz->ijzl\",value, self.F_weight)+ self.F_bias\n",
    "#         key = self.E(key.transpose(-1,-2)).transpose(-1,-2)\n",
    "#         value = self.E(value.transpose(-1,-2)).transpose(-1,-2)\n",
    "        \n",
    "        \n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            x, self.attn_weight = linear_attention(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   attention_type=self.attention_type,\n",
    "                                                   dropout=self.dropout)\n",
    "        elif self.attention_type == 'causal':\n",
    "            assert mask is not None\n",
    "            x, self.attn_weight = causal_linear_attn(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   dropout=self.dropout)\n",
    "        else:\n",
    "            x, self.attn_weight = attention(query, key, value,\n",
    "                                            mask=mask,\n",
    "                                            attention_type=self.attention_type,\n",
    "                                            dropout=self.dropout)\n",
    "\n",
    "        out_dim = self.n_head * self.d_k if pos is None else self.n_head * \\\n",
    "            (self.d_k + self.pos_dim)\n",
    "        att_output = x.transpose(1, 2).contiguous().view(bsz, -1, out_dim)\n",
    "\n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            att_output = self.fc(att_output)\n",
    "            \n",
    "       \n",
    "        return att_output, self.attn_weight\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for param in self.linears.parameters():\n",
    "            if param.ndim > 1:\n",
    "                xavier_uniform_(param, gain=self.xavier_init)\n",
    "                if self.diagonal_weight > 0.0:\n",
    "                    param.data += self.diagonal_weight * \\\n",
    "                        torch.diag(torch.ones(\n",
    "                            param.size(-1), dtype=torch.float))\n",
    "                if self.symmetric_init:\n",
    "                    param.data += param.data.T\n",
    "                    # param.data /= 2.0\n",
    "            else:\n",
    "                constant_(param, 0)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    def _get_norm(self, eps):\n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_V = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_V = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "        else:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_Q = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_Q = self._get_layernorm(self.d_k, self.n_head,\n",
    "                                                  eps=eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_layernorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.LayerNorm(normalized_dim, **kwargs)) for _ in range(n_head)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_instancenorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.InstanceNorm1d(normalized_dim, **kwargs)) for _ in range(n_head)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4805f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svd_SimpleAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model,k=2048,\n",
    "                 pos_dim: int = 1,\n",
    "                 attention_type='fourier',\n",
    "                 dropout=0.1,\n",
    "                 xavier_init=1e-4,\n",
    "                 diagonal_weight=1e-2,\n",
    "                 symmetric_init=False,\n",
    "                 norm=False,\n",
    "                 norm_type='layer',\n",
    "                 eps=1e-5,\n",
    "                 debug=False,\n",
    "                 seq_len = 2048,\n",
    "                ):\n",
    "        super(svd_SimpleAttention, self).__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.attention_type = attention_type\n",
    "        self.d_k = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        self.pos_dim = pos_dim\n",
    "        \n",
    "    \n",
    "        self.xavier_init = xavier_init\n",
    "        self.diagonal_weight = diagonal_weight\n",
    "        self.symmetric_init = symmetric_init\n",
    "\n",
    "        self.add_norm = norm\n",
    "        self.norm_type = norm_type\n",
    "        if norm:\n",
    "            self._get_norm(eps=eps)\n",
    "\n",
    "        if pos_dim > 0:\n",
    "            self.fc = nn.Linear(d_model + n_head*pos_dim, d_model)\n",
    "\n",
    "        self.attn_weight = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.debug = debug\n",
    "        self.E_weight = Parameter(torch.Tensor(seq_len, k))\n",
    "        self.F_weight = Parameter(torch.Tensor(seq_len, k))\n",
    "        self.E_bias = Parameter(torch.Tensor(k, 1))\n",
    "        self.F_bias = Parameter(torch.Tensor(k, 1))\n",
    "        xavier_normal_(self.E_weight)\n",
    "        xavier_normal_(self.F_weight)\n",
    "        xavier_normal_(self.E_bias)\n",
    "        xavier_normal_(self.F_bias)\n",
    "        self.linear = nn.Linear(self.d_k + pos_dim, self.d_k + pos_dim)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, pos=None, mask=None, weight=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        bsz = query.size(0)\n",
    "        if weight is not None:\n",
    "            query, key = weight*query, weight*key \n",
    "\n",
    "        query = query.view(bsz, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        key = key.view(bsz, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "        value = value.view(bsz, -1, self.n_head, self.d_k).transpose(1, 2)            \n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            assert pos.size(-1) == self.pos_dim\n",
    "            pos = pos.unsqueeze(1)\n",
    "            pos = pos.repeat([1, self.n_head, 1, 1])\n",
    "            query, key, value = [torch.cat([pos, x], dim=-1)\n",
    "                                for x in (query, key, value)]  \n",
    "            \n",
    "        query = self.linear(query)\n",
    "        key  = torch.einsum(\"ijkl,kz->ijzl\",key, self.E_weight)+ self.E_bias\n",
    "        value  = torch.einsum(\"ijkl,kz->ijzl\",value, self.F_weight)+ self.F_bias\n",
    "        \n",
    "        if self.add_norm:\n",
    "            if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                value = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_V, (value[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, value = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "            else:\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), query.transpose(-2, -1)\n",
    "\n",
    "                key = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "                query = torch.stack(\n",
    "                    [norm(x) for norm, x in\n",
    "                     zip(self.norm_Q, (query[:, i, ...] for i in range(self.n_head)))], dim=1)\n",
    "\n",
    "                if self.norm_type == 'instance':\n",
    "                    key, query = key.transpose(-2, -1), value.transpose(-2, -1)\n",
    "        \n",
    "        \n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            x, self.attn_weight = linear_attention(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   attention_type=self.attention_type,\n",
    "                                                   dropout=self.dropout)\n",
    "        elif self.attention_type == 'causal':\n",
    "            assert mask is not None\n",
    "            x, self.attn_weight = causal_linear_attn(query, key, value,\n",
    "                                                   mask=mask,\n",
    "                                                   dropout=self.dropout)\n",
    "        else:\n",
    "            x, self.attn_weight = attention(query, key, value,\n",
    "                                            mask=mask,\n",
    "                                            attention_type=self.attention_type,\n",
    "                                            dropout=self.dropout)\n",
    "\n",
    "        out_dim = self.n_head * self.d_k if pos is None else self.n_head * \\\n",
    "            (self.d_k + self.pos_dim)\n",
    "        att_output = x.transpose(1, 2).contiguous().view(bsz, -1, out_dim)\n",
    "\n",
    "        if pos is not None and self.pos_dim > 0:\n",
    "            att_output = self.fc(att_output)\n",
    "            \n",
    "       \n",
    "        return att_output, self.attn_weight\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for param in self.linears.parameters():\n",
    "            if param.ndim > 1:\n",
    "                xavier_uniform_(param, gain=self.xavier_init)\n",
    "                if self.diagonal_weight > 0.0:\n",
    "                    param.data += self.diagonal_weight * \\\n",
    "                        torch.diag(torch.ones(\n",
    "                            param.size(-1), dtype=torch.float))\n",
    "                if self.symmetric_init:\n",
    "                    param.data += param.data.T\n",
    "                    # param.data /= 2.0\n",
    "            else:\n",
    "                constant_(param, 0)\n",
    "                         \n",
    "\n",
    "    def _get_norm(self, eps):\n",
    "        if self.attention_type in ['linear', 'galerkin', 'global']:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_V = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k + self.pos_dim, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_V = self._get_layernorm(self.d_k + self.pos_dim, self.n_head,\n",
    "                                                  eps=eps)\n",
    "        else:\n",
    "            if self.norm_type == 'instance':\n",
    "                self.norm_K = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "                self.norm_Q = self._get_instancenorm(self.d_k, self.n_head,\n",
    "                                                     eps=eps,\n",
    "                                                     affine=True)\n",
    "            elif self.norm_type == 'layer':\n",
    "                self.norm_K = self._get_layernorm(self.d_k + self.pos_dim, self.n_head,\n",
    "                                                  eps=eps)\n",
    "                self.norm_Q = self._get_layernorm(self.d_k + self.pos_dim, self.n_head,\n",
    "                                                  eps=eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_layernorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.LayerNorm(normalized_dim, **kwargs)) for _ in range(n_head)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_instancenorm(normalized_dim, n_head, **kwargs):\n",
    "        return nn.ModuleList(\n",
    "            [copy.deepcopy(nn.InstanceNorm1d(normalized_dim, **kwargs)) for _ in range(n_head)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d998b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=288\n",
    "d_1=1\n",
    "k=5\n",
    "N = 4096\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3002963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c97045",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(1, N, d)\n",
    "K = torch.randn(1, N, d)\n",
    "V = torch.randn(1, N, d)\n",
    "pos = torch.randn(1,N,d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cfa431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_constructor(x):\n",
    "    q = x[0]\n",
    "    k = x[1]\n",
    "    v = x[2]\n",
    "    pos = x[-1]\n",
    "    return {\"query\": torch.randn(q),\n",
    "           \"key\": torch.randn(k),\n",
    "           \"value\": torch.randn(v),\n",
    "           \"pos\": torch.rand(pos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c49fd50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n",
      "original_SimpleAttention(\n",
      "  334.08 k, 100.000% Params, 1.36 GMac, 100.000% MACs, \n",
      "  (linears): ModuleList(\n",
      "    249.7 k, 74.741% Params, 1.02 GMac, 74.740% MACs, \n",
      "    (0): Linear(83.23 k, 24.914% Params, 339.74 MMac, 24.913% MACs, in_features=288, out_features=288, bias=True)\n",
      "    (1): Linear(83.23 k, 24.914% Params, 339.74 MMac, 24.913% MACs, in_features=288, out_features=288, bias=True)\n",
      "    (2): Linear(83.23 k, 24.914% Params, 339.74 MMac, 24.913% MACs, in_features=288, out_features=288, bias=True)\n",
      "  )\n",
      "  (fc): Linear(84.38 k, 25.259% Params, 344.46 MMac, 25.260% MACs, in_features=292, out_features=288, bias=True)\n",
      "  (dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attention_type = \"fourier\"\n",
    "model = original_SimpleAttention(n_head = n_head, d_model = d, pos_dim = d_1, attention_type=attention_type)\n",
    "flops, params = get_model_complexity_info(model, ((1, N, d) ,(1, N, d),(1, N, d),(1, N, d_1)), \n",
    "                                          as_strings=True,input_constructor =input_constructor,\n",
    "                                          print_per_layer_stat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec59fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"fourier\"\n",
    "model = original_SimpleAttention(n_head = n_head, d_model = d, pos_dim = d_1, attention_type=attention_type)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP：开始跑dummy example\n",
    "for _ in range(10):\n",
    "   _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "        ender.record()\n",
    "     # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(f\"{attention_type}:\", mean_syn)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"galerkin\"\n",
    "model = original_SimpleAttention(n_head = n_head, d_model = d, pos_dim = d_1, attention_type=attention_type)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP：开始跑dummy example\n",
    "for _ in range(10):\n",
    "   _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "        ender.record()\n",
    "     # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(f\"{attention_type}:\", mean_syn)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"fourier\"\n",
    "for k in range(3,10):\n",
    "    model = lin_SimpleAttention(n_head = n_head, d_model = d,k = k, pos_dim = d_1, seq_len = N, attention_type='fourier')\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "    dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP：开始跑dummy example\n",
    "    for _ in range(10):\n",
    "       _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "            ender.record()\n",
    "         # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    print(f\"lin {k} {attention_type}:\", mean_syn)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"galerkin\"\n",
    "for k in range(3,10):\n",
    "    model = lin_SimpleAttention(n_head = n_head, d_model = d,k = k, pos_dim = d_1, seq_len = N, attention_type=attention_type)\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "    dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP：开始跑dummy example\n",
    "    for _ in range(10):\n",
    "       _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "            ender.record()\n",
    "         # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    print(f\"lin {k} {attention_type}:\", mean_syn)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"fourier\"\n",
    "for k in range(3,10):\n",
    "    model = svd_SimpleAttention(n_head = n_head, d_model = d,k = k, pos_dim = d_1, seq_len = N, attention_type=attention_type)\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "    dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP：开始跑dummy example\n",
    "    for _ in range(10):\n",
    "       _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "            ender.record()\n",
    "         # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    print(f\"svd {k} {attention_type}:\", mean_syn)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12018706",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_type = \"galerkin\"\n",
    "for k in range(3,10):\n",
    "    model = svd_SimpleAttention(n_head = n_head, d_model = d,k = k, pos_dim = d_1, seq_len = N, attention_type=attention_type)\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    dummy_input= torch.randn(1,N,d,dtype=torch.float).to(device)\n",
    "    dummy_pos = torch.randn(1,N,d_1,dtype=torch.float).to(device)\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "    #GPU-WARM-UP：开始跑dummy example\n",
    "    for _ in range(10):\n",
    "       _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "    # MEASURE PERFORMANCE\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input,dummy_input,dummy_input,dummy_pos)\n",
    "            ender.record()\n",
    "         # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    print(f\"svd {k} {attention_type}:\", mean_syn)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456946cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
